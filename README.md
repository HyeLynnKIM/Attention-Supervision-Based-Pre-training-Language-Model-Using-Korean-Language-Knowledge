# Attention-Supervision-Based-Pre-training-Language-Model-Using-Korean-Language-Knowledge
석사학위청구논문 repo
---
## Abstract ##
사전학습 언어모형은 자연어처리 분야에서 효과적으로 사용하는 모형이다. 셀프 어텐션이라는 메커니즘을 통해 문장 내 문맥을 보다 정확하게 파악하여 의미를 이해하고 해석가능하다. 언어모형은 사전학습 과정을 통하여 학습하는 언어에 대한 기반 지식을 습득하게 되는데, 이때 그 언어에 대한 지식을 추가적으로 넣어준다면 모형의 언어 이해도가 상승하게 될 수 있다.

본 논문에서는 사전학습 언어모형에 한국어 지식을 주입하기 위해 한국어 정보를 알 수 있는 형태소 기반 규칙을 제작하여 학습에 사용한다. 이를 통해 모델이 셀프 어텐션을 업데이트할 때, 형태소 규칙을 따라 업데이트되며 한국어 문장을 보다 효과적으로 이해할 수 있게 된다. 이를 각 태스크마다 미세조정하여 평가했을 때, 기존 사전학습 언어모형보다 형태소 규칙을 적용한 언어모형에서 더 좋은 성능을 확인할 수 있다.
